{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "import random\n",
    "from sklearn.svm import SVC\n",
    "from collections import Counter\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.metrics import accuracy_score\n",
    "from statistics import mode \n",
    "\n",
    "\n",
    "\n",
    "class Cluster:\n",
    "    \n",
    "    vectorize = None\n",
    "    vectorized_train_data = None\n",
    "    train_data_labels = None\n",
    "    K = 5\n",
    "    iterations = 25\n",
    "    def merge_files_data(self,data_path):\n",
    "        train_data_list = []\n",
    "        files = os.listdir(data_path)\n",
    "        count=0\n",
    "#         files = files[:100]\n",
    "        for entry in files:\n",
    "            file_name = entry\n",
    "#             print(file_name)\n",
    "            c_filename = data_path+\"/\"+file_name\n",
    "            cluster_val = file_name.split(\"_\")[1].split(\".\")[0]\n",
    "            f = open(c_filename, \"r\", encoding='latin1')\n",
    "            file_contents = f.read()\n",
    "            a_contents = np.empty((0))\n",
    "            a_contents = np.append(a_contents,file_contents)\n",
    "            a_contents = np.append(a_contents,cluster_val)\n",
    "            train_data_list = np.append(train_data_list, a_contents)\n",
    "\n",
    "        train_data_array = np.reshape(np.asarray(train_data_list) , (len(files),2))\n",
    "        return train_data_array, files\n",
    "    \n",
    "    def preprocess_data(self,train_data_frm):\n",
    "               \n",
    "        rows,cols = train_data_frm.shape\n",
    "              \n",
    "        ''' removing punctuations'''\n",
    "        for i, rows in train_data_frm.iterrows():\n",
    "            val = train_data_frm.iat[i,0]\n",
    "            fval = val.translate(val.maketrans('','', string.punctuation))\n",
    "            train_data_frm.at[i, 'Col1'] = fval\n",
    "        \n",
    "       \n",
    "        ''' removing numbers'''\n",
    "        for i, rows in train_data_frm.iterrows():\n",
    "            val = train_data_frm.iat[i,0]\n",
    "            fval = re.sub(r'\\d+', '', val)\n",
    "            train_data_frm.at[i, 'Col1'] = fval\n",
    "            \n",
    "        ''' converting to lowercase'''\n",
    "        for i, rows in train_data_frm.iterrows():\n",
    "            val = train_data_frm.iat[i,0]\n",
    "            fval = val.lower()\n",
    "            train_data_frm.at[i, 'Col1'] = fval\n",
    "            \n",
    "        '''removing stop words'''\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        for i, rows in train_data_frm.iterrows():\n",
    "            val = train_data_frm.iat[i,0]\n",
    "            tokens = word_tokenize(val)\n",
    "            fval = [i for i in tokens if not i in stop_words]\n",
    "            ffval = \" \"\n",
    "            train_data_frm.at[i, 'Col1'] = ffval.join(fval)\n",
    "            \n",
    "        '''removing whitespaces'''\n",
    "        for i, rows in train_data_frm.iterrows():\n",
    "            val = train_data_frm.iat[i,0]\n",
    "            train_data_frm.at[i, 'Col1'] = val.strip()\n",
    "                                  \n",
    "        return train_data_frm\n",
    "    \n",
    "    def train_validation_split(self,data_frm,validation_data_size):\n",
    "        if isinstance(validation_data_size, float):\n",
    "            validation_data_size=round(validation_data_size * len(data_frm))\n",
    "        indices=data_frm.index.tolist()\n",
    "        valid_indices=random.sample(indices, validation_data_size)\n",
    "        valid_datafrm=data_frm.loc[valid_indices]\n",
    "        train_datafrm=data_frm.drop(valid_indices)\n",
    "        return train_datafrm, valid_datafrm\n",
    "    \n",
    "   \n",
    "    def prepare_data(self,data_frm):\n",
    "        data_labels = data_frm.iloc[:,-1]\n",
    "        data_frm = data_frm.iloc[:,:-1]\n",
    "        return data_frm, data_labels\n",
    "        \n",
    "        \n",
    "    def run_kmeans_algo(self, files):\n",
    "        n = self.vectorized_train_data.shape[0]\n",
    "        n1 = self.vectorized_train_data.shape[1]\n",
    "        train_data_array = self.vectorized_train_data.toarray()\n",
    "        indices=[]\n",
    "        for j in range(self.K): \n",
    "            indices.append(random.randint(0,n)) \n",
    "\n",
    "        cluster_centroids_li = []\n",
    "\n",
    "        for i in range(self.K):\n",
    "            cluster_centroids_li.append(train_data_array[indices[i]])\n",
    "        cluster_centroids = np.asarray(cluster_centroids_li)\n",
    "        \n",
    "       \n",
    "        cluster_dict = {}\n",
    "        cluster_files = {}\n",
    "        cluster_labels = {}\n",
    "        for it in range(self.iterations):\n",
    "#             print(\"iteration \", it)\n",
    "            cluster_centroid_dis = []\n",
    "            cluster_dict = {0:[] , 1:[], 2:[], 3:[], 4:[]}\n",
    "            cluster_files = {0:[] , 1:[], 2:[], 3:[], 4:[]}\n",
    "            cluster_labels = {0:[] , 1:[], 2:[], 3:[], 4:[]}\n",
    "            \n",
    "            for j in range(n):\n",
    "                curr = []\n",
    "                min_dist_clust = np.finfo(np.float64).max\n",
    "                min_clust_num = None\n",
    "                for i in range(self.K):\n",
    "                    ed_dis = np.linalg.norm(train_data_array[j] - cluster_centroids[i])\n",
    "                    val = ed_dis < min_dist_clust\n",
    "                    if val:\n",
    "                        min_dist_clust = ed_dis\n",
    "                        val1 = min_clust_num\n",
    "                        min_clust_num = i\n",
    "                data_val = train_data_array[j]\n",
    "                cluster_dict[min_clust_num].append(data_val)\n",
    "                data_val = files[j]\n",
    "                cluster_files[min_clust_num].append(files[j])\n",
    "                data_val = str(self.train_data_labels[j])\n",
    "                cluster_labels[min_clust_num].append(str(self.train_data_labels[j]))\n",
    "\n",
    "            updated_mean = []\n",
    "            \n",
    "            for i in range(self.K):\n",
    "\n",
    "                cl_array = np.asarray(cluster_dict[i])\n",
    "                cl_mean = cl_array.mean(axis=0)\n",
    "                updated_mean.append(cl_mean)\n",
    "            \n",
    "            cluster_centroids = np.asarray(updated_mean)\n",
    "        cluster_final=[]\n",
    "        final_count=0\n",
    "        for i in range(5):\n",
    "            mod_val=mode(cluster_labels[i])\n",
    "            cluster_final.append(mod_val)\n",
    "            mod_count=cluster_labels[i].count(mod_val)\n",
    "            final_count=final_count+mod_count\n",
    "        print(\"Final Clusters \",cluster_final)\n",
    "        accuracy_val = final_count/len(self.train_data_labels)\n",
    "        print(\"Overall Accuracy \",accuracy_val)\n",
    "                \n",
    "            \n",
    "            \n",
    "    def cluster(self,data_path):\n",
    "        train_data_array,files = self.merge_files_data(data_path)\n",
    "        train_data_frm = pd.DataFrame({'Col1': train_data_array[:, 0], 'Col2': train_data_array[:, 1]})\n",
    "        train_data_frm = self.preprocess_data(train_data_frm)\n",
    "        train_data_frm, self.train_data_labels = self.prepare_data(train_data_frm)\n",
    "        self.vectorize = TfidfVectorizer()\n",
    "        train_data_frm = train_data_frm.values.flatten()\n",
    "        self.vectorized_train_data = self.vectorize.fit_transform(train_data_frm)\n",
    "        self.train_data_labels = self.train_data_labels.values.flatten()\n",
    "        self.vectorized_train_data = self.vectorized_train_data\n",
    "        self.run_kmeans_algo(files)\n",
    "      \n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Clusters  ['1', '5', '4', '3', '2']\n",
      "Overall Accuracy  0.856231884057971\n"
     ]
    }
   ],
   "source": [
    "cluster_algo = Cluster()\n",
    "# You will be given path to a directory which has a list of documents. You need to return a list of cluster labels for those documents\n",
    "predictions = cluster_algo.cluster('/home/jyoti/Documents/SMAI/assign2/Q6/dataset') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
